{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098af403-2e5d-46f9-a322-b6b6b1a4d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ea5603-dc0c-4a62-9b18-98afce0ce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "### custome functions \n",
    "def modify_race_col(row) -> int:\n",
    "    '''\n",
    "    logic to make a compariable filed to HISPALLP_C in the post 2019 datasets\n",
    "    '''\n",
    "    HISPAN_I = row['HISPAN_I']\n",
    "    RACERPI2 = row['RACERPI2']\n",
    "    \n",
    "    if HISPAN_I != 12:\n",
    "        return 1\n",
    "\n",
    "    match RACERPI2:\n",
    "        case 1:\n",
    "            return 2\n",
    "        case 2:\n",
    "            return 3\n",
    "        case 3:\n",
    "            return 5\n",
    "        case 5:\n",
    "            return 99\n",
    "        case 6:\n",
    "            return 7\n",
    "        case _:\n",
    "            return 0\n",
    "\n",
    "### the convert function switch the numeric values to text\n",
    "def convert_region_to_txt(row) ->str:\n",
    "    match row['REGION']:\n",
    "        case 1:\n",
    "            return 'Northeast'\n",
    "        case 2:\n",
    "            return 'Midwest'\n",
    "        case 3:\n",
    "            return 'South'\n",
    "        case 4:\n",
    "            return 'West'\n",
    "            \n",
    "def convert_sex_c_to_txt(row) -> str:\n",
    "    match row['SEX_C']:\n",
    "        case 1:\n",
    "            return 'Male'\n",
    "        case 2:\n",
    "            return 'Female'\n",
    "        case 7:\n",
    "            return 'Refused'\n",
    "        case 8:\n",
    "            return 'Not Ascertained'\n",
    "        case _:\n",
    "            return \"Don't Know\"\n",
    "\n",
    "def convert_question_fields(row, arg1) -> str:\n",
    "    match row[arg1]:\n",
    "        case 1:\n",
    "            return 'Yes'\n",
    "        case 2:\n",
    "            return 'No'\n",
    "        case 7:\n",
    "            return 'Refused'\n",
    "        case 8: \n",
    "            return 'Not Ascertained'\n",
    "        case 9:\n",
    "            return 'Don\\'t Know'\n",
    "        case _:\n",
    "             return 'Don\\'t Know'\n",
    "\n",
    "def convert_HISPALLP_C_to_txt(row) -> str:\n",
    "    match row['HISPALLP_C']:\n",
    "        case 1:\n",
    "            return 'Hispanic'\n",
    "        case 2:\n",
    "            return 'Non-Hispanic White only'\n",
    "        case 3:\n",
    "            return 'Non-Hispanic Black/African American only'\n",
    "        case 4:\n",
    "            return 'Non-Hispanic Asian only'\n",
    "        case 5:\n",
    "            return 'Non-Hispanic AIAN only'\n",
    "        case 6:\n",
    "            return 'Non-Hispanic AIAN and any other group'\n",
    "        case 7:\n",
    "            return 'Other single and multiple races'\n",
    "        case 97:\n",
    "            return 'Refused'\n",
    "        case 98:\n",
    "            return 'Not Ascertained'\n",
    "        case 99:\n",
    "            return \"Don't Know\"\n",
    "\n",
    "\n",
    "def create_data_file_layout_dict() -> dict:\n",
    "    '''\n",
    "    intake text file that filed layouts for .dat \n",
    "    files\n",
    "    '''\n",
    "    layout_dict = dict()\n",
    "\n",
    "    with open('data/file_layouts.txt') as f:\n",
    "        for line in f:\n",
    "            clean_line = line.replace('\\n','').replace('\\t','').split(',')\n",
    "            layout_dict[int(clean_line[0])] = [int(x) for x in clean_line[1:]]\n",
    "\n",
    "    return layout_dict\n",
    "\n",
    "\n",
    "def create_dat_file_vars() -> dict:\n",
    "    '''\n",
    "    create a dictonary that is used to process all the .dat files\n",
    "    this stores all the renames and postions to pull from.\n",
    "    '''\n",
    "    #get the field layout for each file\n",
    "    layout_dict = create_data_file_layout_dict()\n",
    "\n",
    "    #the fields we will select and for what year\n",
    "    dat_fields = [1,2,5,6,8,9,12,13,14,17]\n",
    "    data_2014_fields = [24,25,26,29,31]\n",
    "    data_2011_2013_fields = [24,25,26,29,37]\n",
    "    data_2008_2010_fields = [23,24,25,28,36]\n",
    "    data_2005_2007_fields = [25,33]\n",
    "\n",
    "    #the renames for each file\n",
    "    dat_file_renames = {\n",
    "        1: 'SRVY_YR',\n",
    "        2: 'HHX',\n",
    "        5: 'FMX',\n",
    "        6: 'FPX',\n",
    "        8: 'WTFA_SC',\n",
    "        9: 'REGION',\n",
    "        12: 'SEX',\n",
    "        13: 'HISPAN_I',\n",
    "        14: 'RACERPI2',\n",
    "        17: 'AGE_P',\n",
    "    }\n",
    "\n",
    "    dat_file_2014_renames = {24: 'CWGHT_TC', 25:'CHGHT_TC', 26: 'BMI_SC', 29: 'ADD2', 31: 'AUTISM'}\n",
    "    dat_file_2011_2013_renames = {24: 'CWGHT_TC', 25:'CHGHT_TC', 26: 'BMI_SC', 29: 'ADD2', 37: 'AUTISM'}\n",
    "    dat_file_2008_2010_renames = {23: 'CWGHT_TC', 24:'CHGHT_TC', 25: 'BMI_SC', 28: 'ADD2', 36: 'AUTISM'}\n",
    "    dat_file_2005_2007_renames = {25: 'ADD2', 33: 'AUTISM'}\n",
    "    \n",
    "    \n",
    "    #create final dic used to process the .dat files\n",
    "    dat_files = {\n",
    "        2014:{\n",
    "            'file_nm':'data/child14.dat',\n",
    "            'file_layout':layout_dict[2014],\n",
    "            'select_fields':dat_fields + data_2014_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2014_renames\n",
    "        },\n",
    "        2013:{\n",
    "            'file_nm':'data/child13.dat',\n",
    "            'file_layout':layout_dict[2013],\n",
    "            'select_fields': dat_fields + data_2011_2013_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2011_2013_renames  \n",
    "        },\n",
    "        2012:{\n",
    "            'file_nm':'data/child12.dat',\n",
    "            'file_layout':layout_dict[2012],\n",
    "            'select_fields': dat_fields + data_2011_2013_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2011_2013_renames  \n",
    "        },\n",
    "        2011:{\n",
    "            'file_nm':'data/child11.dat',\n",
    "            'file_layout':layout_dict[2011],\n",
    "            'select_fields': dat_fields + data_2011_2013_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2011_2013_renames  \n",
    "        },\n",
    "        2010:{\n",
    "            'file_nm':'data/child10.dat',\n",
    "            'file_layout':layout_dict[2010],\n",
    "            'select_fields': dat_fields + data_2008_2010_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2008_2010_renames  \n",
    "        },  \n",
    "        2009:{\n",
    "            'file_nm':'data/child09.dat',\n",
    "            'file_layout':layout_dict[2009],\n",
    "            'select_fields': dat_fields + data_2008_2010_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2008_2010_renames  \n",
    "        },  \n",
    "        2008:{\n",
    "            'file_nm':'data/child08.dat',\n",
    "            'file_layout':layout_dict[2008],\n",
    "            'select_fields': dat_fields + data_2008_2010_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2008_2010_renames  \n",
    "        },  \n",
    "        2007:{\n",
    "            'file_nm':'data/child07.dat',\n",
    "            'file_layout':layout_dict[2007],\n",
    "            'select_fields': dat_fields + data_2005_2007_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2005_2007_renames  \n",
    "        },  \n",
    "        2006:{\n",
    "            'file_nm':'data/child06.dat',\n",
    "            'file_layout':layout_dict[2006],\n",
    "            'select_fields': dat_fields + data_2005_2007_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2005_2007_renames  \n",
    "        }, \n",
    "        2005:{\n",
    "            'file_nm':'data/child05.dat',\n",
    "            'file_layout':layout_dict[2005],\n",
    "            'select_fields': dat_fields + data_2005_2007_fields,\n",
    "            'rename_fiedls': dat_file_renames | dat_file_2005_2007_renames  \n",
    "        },  \n",
    "    } \n",
    "\n",
    "    return dat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25c081b-cf9e-4b50-8aed-8c428bd81211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create file list for processing\n",
    "post_19_files = ('data/child23.csv','data/child22.csv','data/child21.csv','data/child20.csv','data/child19.csv',)\n",
    "\n",
    "#pre_19 import vars\n",
    "pre_19_csv = ('data/child18.csv','data/child17.csv','data/child16.csv','data/child15.csv',)\n",
    "\n",
    "pre_19_renames = {\n",
    "    'WTFA_SC':'WTFA_C',\n",
    "    'SEX':'SEX_C',\n",
    "    'AGE_P':'AGEP_C',\n",
    "    'ADD2':'ADHDEV_C',\n",
    "    'ADD2N':'ADHDNW_C',\n",
    "    'CWGHT_TC':'WEIGHTLBTC_C',\n",
    "    'CHGHT_TC':'HEIGHTTC_C',\n",
    "    'BMI_SC':'BMICAT_C',\n",
    "    'AUTISM': 'ASDEV_C',\n",
    "    'AUTISMN': 'ASDNW_C', \n",
    "}\n",
    "\n",
    "\n",
    "dat_files_2005_2014 = create_dat_file_vars()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0dce75-f111-4d51-ab46-bc117111821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     WTFA_SC  REGION  SEX  HISPAN_I  RACERPI2  AGE_P  \\\n",
      "HHX SRVY_YR FMX FPX                                                    \n",
      "13  2014    1   3       6039       4    1         3         1      0   \n",
      "20  2014    1   4       3145       4    2        12         1      2   \n",
      "25  2014    1   4       5497       2    2        12         1     16   \n",
      "29  2014    1   5        592       2    2        12         1      7   \n",
      "34  2014    1   2       6553       1    2        12         2     12   \n",
      "\n",
      "                     CWGHT_TC  CHGHT_TC  BMI_SC  ADD2  AUTISM  \n",
      "HHX SRVY_YR FMX FPX                                            \n",
      "13  2014    1   3         NaN       NaN     NaN   NaN     NaN  \n",
      "20  2014    1   4         NaN       NaN     NaN   2.0     2.0  \n",
      "25  2014    1   4        66.0     135.0  2179.0   2.0     2.0  \n",
      "29  2014    1   5         NaN       NaN     NaN   2.0     2.0  \n",
      "34  2014    1   2        64.0     100.0  1717.0   2.0     2.0  \n",
      "115390\n"
     ]
    }
   ],
   "source": [
    "### load 2005 to 2014 data\n",
    "dat_df = pd.DataFrame()\n",
    "\n",
    "for key, value in dat_files_2005_2014.items():\n",
    "    df = pd.read_fwf(value['file_nm'],widths=value['file_layout'],header=None)\n",
    "    df = df[value['select_fields']]\n",
    "    df = df.rename(columns=value['rename_fiedls'])\n",
    "    dat_df = pd.concat([dat_df,df])\n",
    "\n",
    "\n",
    "dat_df.set_index(['HHX','SRVY_YR','FMX','FPX'],inplace=True)\n",
    "print(dat_df.head())\n",
    "print(len(dat_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c1dd46-a5be-492e-999a-08a6fca901ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40512\n",
      "                     SEX  HISPAN_I  RACERPI2  AGE_P  REGION  WTFA_SC  ADD2  \\\n",
      "HHX SRVY_YR FMX FPX                                                          \n",
      "4   2018    1   3      1        12         1     16       2     6055   2.0   \n",
      "6   2018    1   3      1        12         1      9       3    11581   2.0   \n",
      "8   2018    1   3      1        12         1      3       2     3558   2.0   \n",
      "10  2018    1   4      1        12         1      0       1     4447   NaN   \n",
      "13  2018    1   2      1        12         2     10       3     4567   2.0   \n",
      "\n",
      "                     AUTISM  ADD2N  AUTISMN  CHGHT_TC  CWGHT_TC  BMI_SC  \n",
      "HHX SRVY_YR FMX FPX                                                      \n",
      "4   2018    1   3       2.0    NaN      NaN      96.0     996.0  2687.0  \n",
      "6   2018    1   3       2.0    NaN      NaN       NaN       NaN     NaN  \n",
      "8   2018    1   3       2.0    NaN      NaN       NaN       NaN     NaN  \n",
      "10  2018    1   4       NaN    NaN      NaN       NaN       NaN     NaN  \n",
      "13  2018    1   2       2.0    NaN      NaN       NaN       NaN     NaN  \n"
     ]
    }
   ],
   "source": [
    "### load 2015 to 2018\n",
    "df_2015_2018 = pd.DataFrame()\n",
    "\n",
    "for file in pre_19_csv:\n",
    "\n",
    "    if file == 'data/child15.csv':\n",
    "        cols = ['SRVY_YR','HHX','FMX','FPX','WTFA_SC','REGION','SEX',\n",
    "                'AGE_P','CHGHT_TC','CWGHT_TC','BMI_SC','HISPAN_I',\n",
    "                'RACERPI2','ADD2','AUTISM'\n",
    "                ]\n",
    "    else:\n",
    "        cols = ['SRVY_YR','HHX','FMX','FPX','WTFA_SC','REGION','SEX',\n",
    "                'AGE_P','CHGHT_TC','CWGHT_TC','BMI_SC','HISPAN_I',\n",
    "                'RACERPI2','ADD2','ADD2N','AUTISM','AUTISMN'\n",
    "                ]\n",
    "    \n",
    "    df = pd.read_csv(file,\n",
    "                     sep=',',\n",
    "                     header=0,\n",
    "                     index_col = ['HHX','SRVY_YR','FMX','FPX'],\n",
    "                     usecols=cols\n",
    "                    )\n",
    "\n",
    "    df_2015_2018 = pd.concat([df_2015_2018,df])\n",
    "\n",
    "##print the length\n",
    "print(len(df_2015_2018))\n",
    "print(df_2015_2018.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d11a10e-13d0-4140-8fa4-7ea2e82b5290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([], names=['HHX', 'SRVY_YR', 'FMX', 'FPX'])\n",
      "155902\n"
     ]
    }
   ],
   "source": [
    "### merge 2015 to 2018 and 2005 to 2014 dataframes and strt to do clean up\n",
    "df_2005_2018 = pd.concat([dat_df,df_2015_2018]).reset_index()\n",
    "df_2005_2018.set_index(['HHX','SRVY_YR','FMX','FPX'], inplace=True)\n",
    "\n",
    "\n",
    "duplicates = df_2005_2018.index.duplicated()\n",
    "print(df_2005_2018.index[duplicates])\n",
    "print(len(df_2005_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f37858-8795-4e84-acf9-b6505ca076b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "                     WTFA_C  REGION  SEX_C  AGEP_C  WEIGHTLBTC_C  HEIGHTTC_C  \\\n",
      "HHX SRVY_YR FMX FPX                                                            \n",
      "13  2014    1   3      6039       4      1       0           NaN         NaN   \n",
      "20  2014    1   4      3145       4      2       2           NaN         NaN   \n",
      "25  2014    1   4      5497       2      2      16          66.0       135.0   \n",
      "29  2014    1   5       592       2      2       7           NaN         NaN   \n",
      "34  2014    1   2      6553       1      2      12          64.0       100.0   \n",
      "\n",
      "                     BMICAT_C  ADHDEV_C  ASDEV_C  ADHDNW_C  ASDNW_C  \\\n",
      "HHX SRVY_YR FMX FPX                                                   \n",
      "13  2014    1   3         NaN       NaN      NaN       NaN      NaN   \n",
      "20  2014    1   4         NaN       2.0      2.0       NaN      NaN   \n",
      "25  2014    1   4      2179.0       2.0      2.0       NaN      NaN   \n",
      "29  2014    1   5         NaN       2.0      2.0       NaN      NaN   \n",
      "34  2014    1   2      1717.0       2.0      2.0       NaN      NaN   \n",
      "\n",
      "                     HISPALLP_C  \n",
      "HHX SRVY_YR FMX FPX              \n",
      "13  2014    1   3             1  \n",
      "20  2014    1   4             2  \n",
      "25  2014    1   4             2  \n",
      "29  2014    1   5             2  \n",
      "34  2014    1   2             3  \n",
      "155902\n"
     ]
    }
   ],
   "source": [
    "##create HISPALLP_C\n",
    "df_2005_2018['HISPALLP_C'] = df_2005_2018.apply(modify_race_col, axis=1)\n",
    "print(df_2005_2018['HISPALLP_C'].isnull().sum())\n",
    "\n",
    "##drop columns 'HISPAN_I','RACERPI2' as they are not needed.\n",
    "df_2005_2018.drop(['HISPAN_I','RACERPI2'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "##renaming columns to help combined\n",
    "df_2005_2018 = df_2005_2018.rename(columns=pre_19_renames)\n",
    "print(df_2005_2018.head())\n",
    "print(len(df_2005_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4488585c-9170-43a2-a432-dcb1471039a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         HISPALLP_C  REGION  SEX_C  AGEP_C  ASDNW_C  ASDEV_C  \\\n",
      "HHX     SRVY_YR FMX FPX                                                        \n",
      "H045277 2023    1   1             3       3      2      14      NaN      2.0   \n",
      "H021192 2023    1   1             2       3      2      11      NaN      2.0   \n",
      "H025576 2023    1   1             2       3      1      15      NaN      2.0   \n",
      "H058458 2023    1   1             2       3      2       8      NaN      2.0   \n",
      "H047432 2023    1   1             3       3      1      12      NaN      2.0   \n",
      "\n",
      "                         ADHDNW_C  ADHDEV_C     WTFA_C  BMICAT_C  \\\n",
      "HHX     SRVY_YR FMX FPX                                            \n",
      "H045277 2023    1   1         NaN       2.0  13012.875       NaN   \n",
      "H021192 2023    1   1         NaN       2.0  16680.509       NaN   \n",
      "H025576 2023    1   1         NaN       2.0   5404.923       NaN   \n",
      "H058458 2023    1   1         NaN       2.0   9758.143       NaN   \n",
      "H047432 2023    1   1         1.0       1.0  20404.132       NaN   \n",
      "\n",
      "                         WEIGHTLBTC_C  HEIGHTTC_C  \n",
      "HHX     SRVY_YR FMX FPX                            \n",
      "H045277 2023    1   1             NaN         NaN  \n",
      "H021192 2023    1   1             NaN         NaN  \n",
      "H025576 2023    1   1             NaN         NaN  \n",
      "H058458 2023    1   1             NaN         NaN  \n",
      "H047432 2023    1   1             NaN         NaN  \n",
      "38400\n",
      "MultiIndex([], names=['HHX', 'SRVY_YR', 'FMX', 'FPX'])\n"
     ]
    }
   ],
   "source": [
    "## create the final df with the post 2019 files\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for file in post_19_files:\n",
    "\n",
    "    if file in ('data/child22.csv','data/child20.csv'):\n",
    "        cols = ['HHX','SRVY_YR','WTFA_C','REGION',\n",
    "                'SEX_C','AGEP_C','HEIGHTTC_C',\n",
    "                'WEIGHTLBTC_C','BMICAT_C','HISPALLP_C',\n",
    "                'ADHDEV_C','ADHDNW_C','ASDEV_C','ASDNW_C'\n",
    "                ]\n",
    "    else:\n",
    "        cols = ['HHX','SRVY_YR','WTFA_C','REGION',\n",
    "                'SEX_C','AGEP_C','HISPALLP_C',\n",
    "                'ADHDEV_C','ADHDNW_C','ASDEV_C','ASDNW_C'\n",
    "                ]\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(file,\n",
    "                     sep=',',\n",
    "                     header=0,\n",
    "                     index_col = ['HHX','SRVY_YR'],\n",
    "                     usecols =cols\n",
    "                    )\n",
    "    final_df = pd.concat([final_df,df])\n",
    "\n",
    "\n",
    "\n",
    "final_df['FMX'] = 1\n",
    "final_df['FPX'] = 1\n",
    "\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df.set_index(['HHX','SRVY_YR','FMX','FPX'],inplace=True)\n",
    "\n",
    "duplicates = final_df.index.duplicated()\n",
    "\n",
    "\n",
    "print(final_df.head())\n",
    "print(len(final_df))\n",
    "print(final_df.index[duplicates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de30d52d-b9ee-4036-a48f-74430df8870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         HISPALLP_C  REGION  SEX_C  AGEP_C  ASDNW_C  ASDEV_C  \\\n",
      "HHX     SRVY_YR FMX FPX                                                        \n",
      "H045277 2023    1   1             3       3      2      14      NaN      2.0   \n",
      "H021192 2023    1   1             2       3      2      11      NaN      2.0   \n",
      "H025576 2023    1   1             2       3      1      15      NaN      2.0   \n",
      "H058458 2023    1   1             2       3      2       8      NaN      2.0   \n",
      "H047432 2023    1   1             3       3      1      12      NaN      2.0   \n",
      "\n",
      "                         ADHDNW_C  ADHDEV_C     WTFA_C  BMICAT_C  \\\n",
      "HHX     SRVY_YR FMX FPX                                            \n",
      "H045277 2023    1   1         NaN       2.0  13012.875       NaN   \n",
      "H021192 2023    1   1         NaN       2.0  16680.509       NaN   \n",
      "H025576 2023    1   1         NaN       2.0   5404.923       NaN   \n",
      "H058458 2023    1   1         NaN       2.0   9758.143       NaN   \n",
      "H047432 2023    1   1         1.0       1.0  20404.132       NaN   \n",
      "\n",
      "                         WEIGHTLBTC_C  HEIGHTTC_C  \n",
      "HHX     SRVY_YR FMX FPX                            \n",
      "H045277 2023    1   1             NaN         NaN  \n",
      "H021192 2023    1   1             NaN         NaN  \n",
      "H025576 2023    1   1             NaN         NaN  \n",
      "H058458 2023    1   1             NaN         NaN  \n",
      "H047432 2023    1   1             NaN         NaN  \n",
      "194302\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.concat([final_df,df_2005_2018], axis = 0).reset_index()\n",
    "final_df.set_index(['HHX','SRVY_YR','FMX','FPX'],inplace=True)\n",
    "print(final_df.head())\n",
    "print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "543365fc-344d-4d34-a30e-548bdc0f8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### reformat fields\n",
    "final_df['REGION'] = final_df.apply(convert_region_to_txt, axis=1)\n",
    "final_df['SEX_C'] = final_df.apply(convert_sex_c_to_txt, axis=1)\n",
    "final_df['HISPALLP_C'] = final_df.apply(convert_HISPALLP_C_to_txt, axis=1)\n",
    "final_df['ADHDEV_C'] = final_df.apply(convert_question_fields, axis=1, args=('ADHDEV_C',))\n",
    "final_df['ADHDNW_C'] = final_df.apply(convert_question_fields, axis=1, args=('ADHDNW_C',))\n",
    "final_df['ASDEV_C'] = final_df.apply(convert_question_fields, axis=1, args=('ASDEV_C',))\n",
    "final_df['ASDNW_C'] = final_df.apply(convert_question_fields, axis=1, args=('ASDNW_C',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f93c64e-7bca-4f6e-ace2-87b98f520898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>HISPALLP_C</th>\n",
       "      <th>REGION</th>\n",
       "      <th>SEX_C</th>\n",
       "      <th>AGEP_C</th>\n",
       "      <th>ASDNW_C</th>\n",
       "      <th>ASDEV_C</th>\n",
       "      <th>ADHDNW_C</th>\n",
       "      <th>ADHDEV_C</th>\n",
       "      <th>WTFA_C</th>\n",
       "      <th>BMICAT_C</th>\n",
       "      <th>WEIGHTLBTC_C</th>\n",
       "      <th>HEIGHTTC_C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHX</th>\n",
       "      <th>SRVY_YR</th>\n",
       "      <th>FMX</th>\n",
       "      <th>FPX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H045277</th>\n",
       "      <th>2023</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>Non-Hispanic Black/African American only</td>\n",
       "      <td>South</td>\n",
       "      <td>Female</td>\n",
       "      <td>14</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>13012.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H021192</th>\n",
       "      <th>2023</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>Non-Hispanic White only</td>\n",
       "      <td>South</td>\n",
       "      <td>Female</td>\n",
       "      <td>11</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>16680.509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H025576</th>\n",
       "      <th>2023</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>Non-Hispanic White only</td>\n",
       "      <td>South</td>\n",
       "      <td>Male</td>\n",
       "      <td>15</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>5404.923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H058458</th>\n",
       "      <th>2023</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>Non-Hispanic White only</td>\n",
       "      <td>South</td>\n",
       "      <td>Female</td>\n",
       "      <td>8</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>9758.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H047432</th>\n",
       "      <th>2023</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>Non-Hispanic Black/African American only</td>\n",
       "      <td>South</td>\n",
       "      <td>Male</td>\n",
       "      <td>12</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>20404.132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       HISPALLP_C REGION  \\\n",
       "HHX     SRVY_YR FMX FPX                                                    \n",
       "H045277 2023    1   1    Non-Hispanic Black/African American only  South   \n",
       "H021192 2023    1   1                     Non-Hispanic White only  South   \n",
       "H025576 2023    1   1                     Non-Hispanic White only  South   \n",
       "H058458 2023    1   1                     Non-Hispanic White only  South   \n",
       "H047432 2023    1   1    Non-Hispanic Black/African American only  South   \n",
       "\n",
       "                          SEX_C  AGEP_C     ASDNW_C ASDEV_C    ADHDNW_C  \\\n",
       "HHX     SRVY_YR FMX FPX                                                   \n",
       "H045277 2023    1   1    Female      14  Don't Know      No  Don't Know   \n",
       "H021192 2023    1   1    Female      11  Don't Know      No  Don't Know   \n",
       "H025576 2023    1   1      Male      15  Don't Know      No  Don't Know   \n",
       "H058458 2023    1   1    Female       8  Don't Know      No  Don't Know   \n",
       "H047432 2023    1   1      Male      12  Don't Know      No         Yes   \n",
       "\n",
       "                        ADHDEV_C     WTFA_C  BMICAT_C  WEIGHTLBTC_C  \\\n",
       "HHX     SRVY_YR FMX FPX                                               \n",
       "H045277 2023    1   1         No  13012.875       NaN           NaN   \n",
       "H021192 2023    1   1         No  16680.509       NaN           NaN   \n",
       "H025576 2023    1   1         No   5404.923       NaN           NaN   \n",
       "H058458 2023    1   1         No   9758.143       NaN           NaN   \n",
       "H047432 2023    1   1        Yes  20404.132       NaN           NaN   \n",
       "\n",
       "                         HEIGHTTC_C  \n",
       "HHX     SRVY_YR FMX FPX              \n",
       "H045277 2023    1   1           NaN  \n",
       "H021192 2023    1   1           NaN  \n",
       "H025576 2023    1   1           NaN  \n",
       "H058458 2023    1   1           NaN  \n",
       "H047432 2023    1   1           NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb10f9a-804a-4b61-8be7-8d72e633e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## export the file for use latter\n",
    "final_df.to_csv('data/final_data_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "564bf7b6-5c6d-4412-9fbb-a3ea9856a943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>HISPALLP_C</th>\n",
       "      <th>REGION</th>\n",
       "      <th>SEX_C</th>\n",
       "      <th>AGEP_C</th>\n",
       "      <th>ASDNW_C</th>\n",
       "      <th>ASDEV_C</th>\n",
       "      <th>ADHDNW_C</th>\n",
       "      <th>ADHDEV_C</th>\n",
       "      <th>WTFA_C</th>\n",
       "      <th>BMICAT_C</th>\n",
       "      <th>WEIGHTLBTC_C</th>\n",
       "      <th>HEIGHTTC_C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHX</th>\n",
       "      <th>SRVY_YR</th>\n",
       "      <th>FMX</th>\n",
       "      <th>FPX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [HISPALLP_C, REGION, SEX_C, AGEP_C, ASDNW_C, ASDEV_C, ADHDNW_C, ADHDEV_C, WTFA_C, BMICAT_C, WEIGHTLBTC_C, HEIGHTTC_C]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[final_df['WTFA_C'].isnull()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
